{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intermediate_hellaswag\n",
    "This notebook takes our hellaswag dataset and trains an intermediate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, update working directory to parent so that we may use our custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "# os.getcwd( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import params\n",
    "from utils import *\n",
    "from trainer import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "# suppress model warning\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# set logging level\n",
    "import logging\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set general seeds\n",
    "set_seeds(1)\n",
    "\n",
    "# set dataloader generator seed\n",
    "g = torch.Generator()\n",
    "g.manual_seed(1)\n",
    "\n",
    "# set params for this model\n",
    "params.num_labels = 4\n",
    "params.output_dir = \"model_saves/intermediate_cosmos_01\"\n",
    "\n",
    "# Ensure we're on an ARM environment if necessary.\n",
    "platform_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosmos QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_datasets = load_from_disk(\"data/inter_CosmosQA/itesd_cosmosqa_balanced.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one(example):\n",
    "    print(f\"Context: {example['context']}\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"  A - {example['answer0']}\")\n",
    "    print(f\"  B - {example['answer1']}\")\n",
    "    print(f\"  C - {example['answer2']}\")\n",
    "    print(f\"  D - {example['answer3']}\")\n",
    "    print(f\"\\nGround truth: option {['A', 'B', 'C', 'D'][example['label']]}\")\n",
    "\n",
    "show_one(cosmos_datasets[\"train\"][50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding_dict\n",
    "encoded_datasets = cosmos_datasets.map(cosmos_preprocessing, batched=True)\n",
    "\n",
    "encoded_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double-Check input_id lengths\n",
    "We're performing this check to ensure that 256 max token length is sufficient for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = encoded_datasets[\"train\"]['input_ids']\n",
    "\n",
    "lengths = []\n",
    "for i in train_ids:\n",
    "    for j in i:\n",
    "        lengths.append(len(j))\n",
    "\n",
    "print(len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View input Structure\n",
    "\n",
    "The inputs are four copies of cxt_a and ctx_b each strung together with one ending option. They start with the \\<s> BOS token, which may act as the CLS token instead, and are separated with the \\</s> token--end of sequence or separator token.\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/roberta\n",
    "https://stackoverflow.com/questions/61465223/roberta-tokenization-of-multiple-sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_one(cosmos_datasets[\"train\"][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params.tokenizer.decode(encoded_datasets['train'][\"input_ids\"][100][0]))\n",
    "print(params.tokenizer.decode(encoded_datasets['train'][\"input_ids\"][100][1]))\n",
    "print(params.tokenizer.decode(encoded_datasets['train'][\"input_ids\"][100][2]))\n",
    "print(params.tokenizer.decode(encoded_datasets['train'][\"input_ids\"][100][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_datasets['train'][\"input_ids\"][100][0])\n",
    "print(encoded_datasets['train'][\"attention_mask\"][100][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "\n",
    "train_number_samples = len(encoded_datasets['train'])\n",
    "val_number_samples = len(encoded_datasets['validation'])\n",
    "\n",
    "train_features = [{k: v for k, v in encoded_datasets[\"train\"][i].items() if k in accepted_keys} for i in range(train_number_samples)]\n",
    "validate_features = [{k: v for k, v in encoded_datasets[\"validation\"][i].items() if k in accepted_keys} for i in range(val_number_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders w collation\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_features,\n",
    "            sampler = RandomSampler(train_features),\n",
    "            batch_size = params.batch_size,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "            collate_fn=mc_collate\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            validate_features,\n",
    "            sampler = RandomSampler(validate_features),\n",
    "            batch_size = params.batch_size,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "            collate_fn=mc_collate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view an example from the dataloader\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: if continuing from checkpoint, continue to next section\n",
    "\n",
    "Download transformers.RobertaForSequenceClassification, which is a RoBERTa model with a linear layer for sentence classification (or regression) on top of the pooled output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RobertaForSequenceClassification model\n",
    "model = RobertaForMultipleChoice.from_pretrained('roberta-base',\n",
    "                                                  num_labels = params.num_labels,\n",
    "                                                  output_attentions = False,\n",
    "                                                  output_hidden_states = False,\n",
    "                                                    )\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 4, 256), dtypes=['torch.IntTensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model to device, initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(params.device)\n",
    "# print(f\"Trained Dataset: {dataset_path}\")\n",
    "print(f\"Device: {params.device}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=params.learning_rate) #roberta\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  device=params.device,\n",
    "                  tokenizer=params.tokenizer,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  validation_dataloader=validation_dataloader,\n",
    "                  epochs=params.epochs,\n",
    "                  optimizer=optimizer,\n",
    "                  val_loss_fn=params.val_loss_fn,\n",
    "                  num_labels=params.num_labels,\n",
    "                  output_dir=params.output_dir,\n",
    "                  save_freq=params.save_freq,\n",
    "                  checkpoint_freq=params.checkpoint_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Training from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RobertaForSequenceClassification model\n",
    "model = RobertaForMultipleChoice.from_pretrained('roberta-base',\n",
    "                                                  num_labels = params.num_labels,\n",
    "                                                  output_attentions = False,\n",
    "                                                  output_hidden_states = False,\n",
    "                                                    )\n",
    "\n",
    "model.to(params.device)\n",
    "print(f\"Device: {params.device}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=params.learning_rate) #roberta\n",
    "\n",
    "checkpoint_load = \"model_saves/intermediate_cosmos_01/E07_A0.68_F0.68/checkpoint.pt\"\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  device=params.device,\n",
    "                  tokenizer=params.tokenizer,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  validation_dataloader=validation_dataloader,\n",
    "                  epochs=params.epochs,\n",
    "                  optimizer=optimizer,\n",
    "                  val_loss_fn=params.val_loss_fn,\n",
    "                  num_labels=params.num_labels,\n",
    "                  output_dir=params.output_dir,\n",
    "                  save_freq=params.save_freq,\n",
    "                  checkpoint_freq=params.checkpoint_freq, \n",
    "                  checkpoint_load=checkpoint_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'model_saves/intermediate_cosmos_01/E04_A0.7_F0.7/'\n",
    "# PATH = 'model_saves/intermediate_hellaswag_01/E04_A0.61_F0.61'\n",
    "model = RobertaForMultipleChoice.from_pretrained(PATH, local_files_only=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(PATH, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos = load_dataset(\"cosmos_qa\")\n",
    "\n",
    "cosmos_test = cosmos['test']\n",
    "\n",
    "cosmos_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_test['label'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = cosmos_test.map(cosmos_preprocessing, batched=True, fn_kwargs={\"eval\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_number_samples = len(encoded_dataset)\n",
    "accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "test_features = [{k: v for k, v in encoded_dataset[i].items() if k in accepted_keys} for i in range(test_number_samples)]\n",
    "true_labels = [k['label'] for k in test_features]\n",
    "\n",
    "model.to(params.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "# run tests and append to output\n",
    "with tqdm(test_features, unit=\"test\") as prog:\n",
    "    for step, test in enumerate(prog):\n",
    "        prog.set_description(f\"\\tTest {step+1}\")\n",
    "                \n",
    "        labels = torch.tensor(0).unsqueeze(0)\n",
    "        test_input = {\"input_ids\": torch.IntTensor(test['input_ids']), \"attention_mask\": torch.IntTensor(test['attention_mask'])}\n",
    "        outputs = model(**{k: v.unsqueeze(0).to(params.device) for k, v in test_input.items()}, labels=labels.to(params.device))\n",
    "        logits = outputs.logits\n",
    "        predicted_class = logits.argmax().item()\n",
    "        predictions.append(int(predicted_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('itesd_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c42b54925bdca82cdb5059acc0a21648e00763ff265e64872b54aa656b5d9d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
