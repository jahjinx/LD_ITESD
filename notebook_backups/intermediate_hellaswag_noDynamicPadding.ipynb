{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intermediate_hellaswag\n",
    "This notebook takes our hellaswag dataset and trains an intermediate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform Check\n",
    "Ensure we're on an ARM environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're Armed: macOS-13.0-arm64-i386-64bit\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "if platform.platform() == 'macOS-13.0-arm64-i386-64bit':\n",
    "    print(f\"We're Armed: {platform.platform()}\")\n",
    "else:\n",
    "    print(f\"WARNING! NOT ARMED: {platform.platform()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, update working directory to parent so that we may use our custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jarradjinx/Library/Mobile Documents/com~apple~CloudDocs/EDU_leeds/LD_research/LD_ITESD'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "os.chdir('..')\n",
    "os.getcwd( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2987760f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import params\n",
    "from utils import *\n",
    "from trainer import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "from datasets import load_from_disk, load_metric\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# suppress model warning\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# set logging level\n",
    "import logging\n",
    "logging.basicConfig(level='INFO')\n",
    "\n",
    "# set general seeds\n",
    "set_seeds(1)\n",
    "\n",
    "# set dataloader generator seed\n",
    "g = torch.Generator()\n",
    "g.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hellaswag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"../data/inter_HellaSwag/hellaswag.hf\"\n",
    "hellaswag_datasets = load_from_disk(\"data/inter_HellaSwag/hellaswag.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'endings', 'source_id', 'split', 'split_type', 'label', 'ending0', 'ending1', 'ending2', 'ending3'],\n",
       "        num_rows: 39905\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'endings', 'source_id', 'split', 'split_type', 'label', 'ending0', 'ending1', 'ending2', 'ending3'],\n",
       "        num_rows: 10042\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellaswag_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: A group of athletes row on canoes during a race in between buoys on a waterway.\n",
      "  A - the men pass over a wooden structure in the river.\n",
      "  B - the men paddle while crashing through endless waves in the river.\n",
      "  C - the men cross the final numbered buoys and glide while slowing down after the race.\n",
      "  D - the men go over large cliffs into a lagoon.\n",
      "\n",
      "Ground truth: option C\n"
     ]
    }
   ],
   "source": [
    "def show_one(example):\n",
    "    print(f\"Context: {example['ctx_a']}\")\n",
    "    print(f\"  A - {example['ctx_b']} {example['ending0']}\")\n",
    "    print(f\"  B - {example['ctx_b']} {example['ending1']}\")\n",
    "    print(f\"  C - {example['ctx_b']} {example['ending2']}\")\n",
    "    print(f\"  D - {example['ctx_b']} {example['ending3']}\")\n",
    "    print(f\"\\nGround truth: option {['A', 'B', 'C', 'D'][example['label']]}\")\n",
    "\n",
    "show_one(hellaswag_datasets[\"train\"][250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the dataset\n",
    "examples = hellaswag_datasets[\"train\"][:50]\n",
    "labels = hellaswag_datasets['train']['label'][:50]\n",
    "\n",
    "# use full dataset\n",
    "# for some reason, setting these values using slice notation makes preprocessing MUCH faster\n",
    "# examples = hellaswag_datasets[\"train\"][:]\n",
    "# labels = hellaswag_datasets['train']['label'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "\n",
    "# for sample in examples:\n",
    "encoding_dict = mc_preprocessing(examples, params.tokenizer, ending_names)\n",
    "\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dict\n",
    "\n",
    "token_id = encoding_dict['input_ids']\n",
    "attention_masks = encoding_dict['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = torch.stack(token_id, 0)\n",
    "attention_masks = torch.stack(attention_masks, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2\n",
    "\n",
    "# Indices of the train and validation splits stratified by labels\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = val_ratio,\n",
    "    shuffle = True,\n",
    "    stratify = labels,\n",
    "    random_state=1)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "                                                \n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = params.batch_size,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = params.batch_size,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(iter(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[   0,  170,  192,  ...,    1,    1,    1],\n",
       "          [   0,  170,  192,  ...,    1,    1,    1],\n",
       "          [   0,  170,  192,  ...,    1,    1,    1],\n",
       "          [   0,  170,  192,  ...,    1,    1,    1]],\n",
       " \n",
       "         [[   0, 4763, 1413,  ...,    1,    1,    1],\n",
       "          [   0, 4763, 1413,  ...,    1,    1,    1],\n",
       "          [   0, 4763, 1413,  ...,    1,    1,    1],\n",
       "          [   0, 4763, 1413,  ...,    1,    1,    1]],\n",
       " \n",
       "         [[   0,  133,  693,  ...,    1,    1,    1],\n",
       "          [   0,  133,  693,  ...,    1,    1,    1],\n",
       "          [   0,  133,  693,  ...,    1,    1,    1],\n",
       "          [   0,  133,  693,  ...,    1,    1,    1]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[   0,  250,  410,  ...,    1,    1,    1],\n",
       "          [   0,  250,  410,  ...,    1,    1,    1],\n",
       "          [   0,  250,  410,  ...,    1,    1,    1],\n",
       "          [   0,  250,  410,  ...,    1,    1,    1]],\n",
       " \n",
       "         [[   0, 1620,   37,  ...,    1,    1,    1],\n",
       "          [   0, 1620,   37,  ...,    1,    1,    1],\n",
       "          [   0, 1620,   37,  ...,    1,    1,    1],\n",
       "          [   0, 1620,   37,  ...,    1,    1,    1]],\n",
       " \n",
       "         [[   0,  133, 5645,  ...,    1,    1,    1],\n",
       "          [   0,  133, 5645,  ...,    1,    1,    1],\n",
       "          [   0,  133, 5645,  ...,    1,    1,    1],\n",
       "          [   0,  133, 5645,  ...,    1,    1,    1]]]),\n",
       " tensor([[[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]]]),\n",
       " tensor([3, 0, 2, 0, 1, 0, 1, 2, 2, 0, 3, 1, 3, 3, 1, 0])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download transformers.RobertaForSequenceClassificatio, which is a RoBERTa model with a linear layer for sentence classification (or regression) on top of the pooled output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "RobertaForMultipleChoice                                     [1, 4]                    --\n",
       "├─RobertaModel: 1-1                                          [4, 768]                  --\n",
       "│    └─RobertaEmbeddings: 2-1                                [4, 256, 768]             --\n",
       "│    │    └─Embedding: 3-1                                   [4, 256, 768]             38,603,520\n",
       "│    │    └─Embedding: 3-2                                   [4, 256, 768]             768\n",
       "│    │    └─Embedding: 3-3                                   [4, 256, 768]             394,752\n",
       "│    │    └─LayerNorm: 3-4                                   [4, 256, 768]             1,536\n",
       "│    │    └─Dropout: 3-5                                     [4, 256, 768]             --\n",
       "│    └─RobertaEncoder: 2-2                                   [4, 256, 768]             --\n",
       "│    │    └─ModuleList: 3-6                                  --                        85,054,464\n",
       "│    └─RobertaPooler: 2-3                                    [4, 768]                  --\n",
       "│    │    └─Linear: 3-7                                      [4, 768]                  590,592\n",
       "│    │    └─Tanh: 3-8                                        [4, 768]                  --\n",
       "├─Dropout: 1-2                                               [4, 768]                  --\n",
       "├─Linear: 1-3                                                [4, 1]                    769\n",
       "==============================================================================================================\n",
       "Total params: 124,646,401\n",
       "Trainable params: 124,646,401\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 498.59\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 855.66\n",
       "Params size (MB): 498.59\n",
       "Estimated Total Size (MB): 1354.25\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
    "\n",
    "\n",
    "# Load the RobertaForSequenceClassification model\n",
    "model = RobertaForMultipleChoice.from_pretrained('roberta-base',\n",
    "                                                  num_labels = params.num_labels,\n",
    "                                                  output_attentions = False,\n",
    "                                                  output_hidden_states = False,\n",
    "                                                    )\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 4, 256), dtypes=['torch.IntTensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model to device, initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "model.to(params.device)\n",
    "# print(f\"Trained Dataset: {dataset_path}\")\n",
    "print(f\"Device: {params.device}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=params.learning_rate) #roberta\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  device=params.device,\n",
    "                  tokenizer=params.tokenizer,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  validation_dataloader=validation_dataloader,\n",
    "                  epochs=params.epochs,\n",
    "                  optimizer=optimizer,\n",
    "                  val_loss_fn=params.val_loss_fn,\n",
    "                  num_labels=params.num_labels,\n",
    "                  notify=params.notify,\n",
    "                  phone_number=params.phone_number,\n",
    "                  save_dir=params.save_dir,\n",
    "                  model_name=params.model_name, \n",
    "                  save_freq=params.save_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/3 [00:00<?, ?batch/s]/Users/jarradjinx/opt/anaconda3/envs/itesd_env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1570: UserWarning: The operator 'aten::cumsum.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
      "Epoch 1:  33%|███▎      | 1/3 [00:32<01:04, 32.07s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/EDU_leeds/LD_research/LD_ITESD/trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m train_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(b_input_ids, \n\u001b[1;32m     64\u001b[0m                     token_type_ids \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \n\u001b[1;32m     65\u001b[0m                     attention_mask \u001b[39m=\u001b[39m b_input_mask, \n\u001b[1;32m     66\u001b[0m                     labels \u001b[39m=\u001b[39m b_labels)\n\u001b[1;32m     68\u001b[0m \u001b[39m# training_loss = compute_loss(train_output.logits, b_labels) # custom loss\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m train_output\u001b[39m.\u001b[39;49mloss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     72\u001b[0m \u001b[39m# training_loss.backward() # custom  oss\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/itesd_env/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/itesd_env/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('itesd_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c42b54925bdca82cdb5059acc0a21648e00763ff265e64872b54aa656b5d9d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
